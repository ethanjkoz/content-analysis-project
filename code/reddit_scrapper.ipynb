{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "import util\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.webdriver.common.by import By\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dict_to_csv(data_iter, index_filename):\n",
    "#     \"\"\"\n",
    "#     Creates a csv file from a dictionary, mapping integers to indvidual words\n",
    "#     Inputs:\n",
    "#         data_iter (iterable): maps course index to a set of words\n",
    "#         index_file_name (str): name of the csv file\n",
    "#     Returns: None, creates a csv file\n",
    "#     \"\"\"\n",
    "#     with open(index_filename, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "#         csv_writer = csv.writer(csvfile, delimiter='|')\n",
    "#         for course_index, word_set in data_iter.items():\n",
    "#             for word in word_set:\n",
    "#                 csv_writer.writerow([course_index, word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPERS ###\n",
    "# OVERALL TASK IS TO SCRAPE REDDIT FOR POSTS AND COMMENTS\n",
    "\n",
    "## SHOULD CHANGE THE ATTRS FUNCs TO ADD SUBREDDIT##\n",
    "### HELPERS ###\n",
    "# OVERALL TASK IS TO SCRAPE REDDIT FOR POSTS AND COMMENTS\n",
    "\n",
    "## SHOULD CHANGE THE ATTRS FUNCs TO ADD SUBREDDIT##\n",
    "\n",
    "## SHOULD ALSO ADD WHETHER OR NOT IS COMMENT AND LINK TO ORIGINAL POST\n",
    "\n",
    "def get_next_page(soup):\n",
    "    \"\"\"\n",
    "    Find the next page of a subreddit\n",
    "    Input: soup (bs4 soup): an html soup of a subreddit page\n",
    "    Returns str or None if it finds a next page\n",
    "    \"\"\"\n",
    "    next_page = soup.find(\"span\", class_=\"next-button\")\n",
    "    if next_page:\n",
    "        return next_page.find(\"a\").get(\"href\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def reddit_crawler(domain_URL, r_headers, wait_times, max_pages, csv_filename):\n",
    "    \"\"\"\n",
    "    Crawls a reddit by page, obtaining a link for each page on subreddit\n",
    "    returns a tuple of the visited_pages, visited_pages_soup, urls_to_visit\n",
    "    Inputs:\n",
    "        domain_URL (str): the URL of the subreddit\n",
    "        r_headers (dict): headers for the request\n",
    "        wait_times (tuple of ints): min and max time to wait between requests\n",
    "        max_pages (int): limits the number of pages to crawl\n",
    "        csv_filename (str): the name of the csv file to store page soups\n",
    "    Returns: \n",
    "        visited_pages (list) of page urls\n",
    "        visited_pages_soup (list) of page soups (same len as visited_pages)\n",
    "        urls_to_visit (list) of all comment urls on every page \n",
    "        \n",
    "        also creates a .csv file with the link and soup for the webpage\n",
    "    \"\"\"\n",
    "    visited_pages = []\n",
    "    urls_to_visit = []\n",
    "    visited_pages_soup = []\n",
    "    num_pages_visited = 0\n",
    "    curr_url = domain_URL # initialize curr_url as domain_URL\n",
    "    min_time, max_time = wait_times\n",
    "    \n",
    "    # create a csv file\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Check if the file is empty (write header if needed)\n",
    "        if csvfile.tell() == 0:\n",
    "            csv_writer.writerow([\"link\", \"soup\"])\n",
    "        \n",
    "        # while the number of pages is less than the max\n",
    "        while num_pages_visited < max_pages:\n",
    "            # add current page to visited pages\n",
    "            num_pages_visited += 1\n",
    "\n",
    "            print(\"Pages visited:\", num_pages_visited)\n",
    "\n",
    "            # get request for webpage\n",
    "            request = requests.get(curr_url, headers=r_headers)\n",
    "\n",
    "            # give the server a break\n",
    "            time.sleep(random.uniform(min_time, max_time))\n",
    "\n",
    "            # if request is valid\n",
    "            if request.status_code == 200:\n",
    "                soup_page = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                \n",
    "                # update the CSV with this new url and page\n",
    "                csv_writer.writerow([curr_url, soup_page])\n",
    "\n",
    "                # updated visited pages soup list\n",
    "                visited_pages_soup.append(soup_page)\n",
    "                visited_pages.append(curr_url)\n",
    "\n",
    "                # find all the comment links\n",
    "                links_html = soup_page.find_all(\"a\", class_ = re.compile(r\"bylink comments\"))\n",
    "                links = [link.get(\"href\") for link in links_html]\n",
    "                # add links to visited_urls\n",
    "                urls_to_visit += links\n",
    "\n",
    "                next_page = get_next_page(soup_page)\n",
    "                if next_page:\n",
    "                    curr_url = next_page\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # if unable to get current URL request, return visited URLs\n",
    "            else:\n",
    "                print(f\"Response {request.status_code}. Couldn't get URL: {curr_url}.\")\n",
    "                break\n",
    "        \n",
    "    return visited_pages, visited_pages_soup, urls_to_visit\n",
    "\n",
    "\n",
    "def find_post_attrs(post):\n",
    "    '''\n",
    "    For posts, find important attributes for a Reddit post,\n",
    "    Returns a list of attributes for a post.\n",
    "    Inputs:\n",
    "        post (bs4 soup): a soup of a reddit post\n",
    "    Returns a list of the attributes of the post\n",
    "    '''\n",
    "    if not post:\n",
    "        return [None]*9\n",
    "    # find the user\n",
    "    try:\n",
    "        user = post.find(\"div\").get(\"data-author\")\n",
    "    except:\n",
    "        user = None\n",
    "    try:\n",
    "        user_flair = post.find(\"p\", class_=\"tagline\").find(\"span\", class_=re.compile(\"flair\")).text\n",
    "    except:\n",
    "        user_flair = None\n",
    "\n",
    "    try:\n",
    "        title = post.find(\"a\", class_=re.compile(\"title\")).text\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        post_text = post.find(\"div\", class_=\"md\").text\n",
    "    except:\n",
    "        post_text = None\n",
    "    \n",
    "    try:\n",
    "        post_date = post.find(\"div\").find(\"time\").get(\"datetime\")\n",
    "    except:\n",
    "        post_date = None\n",
    "\n",
    "    try:\n",
    "        post_flair = post.find(\"p\", class_=re.compile(\"title\")).find(\"span\", class_=re.compile(\"flair\")).text\n",
    "    except:\n",
    "        post_flair = None\n",
    "\n",
    "    try:\n",
    "        score = post.find(\"div\", class_=\"score unvoted\").text\n",
    "    except:\n",
    "        score = None\n",
    "\n",
    "    try: \n",
    "        n_comments = post.find(\"div\").get(\"data-comments-count\")\n",
    "\n",
    "    except:\n",
    "        n_comments = None\n",
    "\n",
    "    try:\n",
    "        link = post.find(\"a\", class_=re.compile(r\"comments\")).get(\"href\")\n",
    "    except:\n",
    "        link = None\n",
    "    post_attrs = [user, user_flair, title, post_text, \n",
    "                  post_date, post_flair, score, n_comments, link]\n",
    "\n",
    "    return post_attrs\n",
    "\n",
    "\n",
    "def find_com_attrs(comment, link):\n",
    "    '''\n",
    "    For comments, find important attributes for a Reddit comment,\n",
    "    Returns a list of attributes for a comment.\n",
    "    Inputs:\n",
    "        comment (bs4 soup): a soup of a reddit comment\n",
    "        link (str): the link to the original post\n",
    "    Returns a list of the attributes of the comment\n",
    "    '''\n",
    "    if not comment:\n",
    "        return [None]*9\n",
    "    try:\n",
    "        user = comment.find(\"a\", class_=re.compile(\"author\")).text\n",
    "    except:\n",
    "        user = None\n",
    "    try:\n",
    "        user_flair = comment.find(\"span\", class_= re.compile(r\"flair\")).get(\"title\")\n",
    "    except:\n",
    "        user_flair = None\n",
    "    try:\n",
    "        com_text = comment.find(\"div\", class_=\"md\").text\n",
    "    except:\n",
    "        com_text = None\n",
    "    try:\n",
    "        com_date = comment.find(\"time\").get(\"datetime\")\n",
    "    except:\n",
    "        com_date = None\n",
    "\n",
    "    try:\n",
    "        com_score = int(comment.find(\"span\", class_=\"score unvoted\").get(\"title\", 0))\n",
    "    except:    \n",
    "        com_score = None\n",
    "\n",
    "    com_attrs = [user, user_flair, None, com_text, \n",
    "                 com_date, None, com_score, None, link]\n",
    "    \n",
    "    return com_attrs\n",
    "\n",
    "\n",
    "def reddit_scraper(page_soups, r_headers, wait_times, csv_filename, max_com):\n",
    "    '''\n",
    "    Scrape all the post and comment text for a given subreddit page, return\n",
    "    all post soups\n",
    "\n",
    "    Inputs:\n",
    "        page_soups (list): list of soups for each page\n",
    "        r_headers (dict): headers for the request\n",
    "        wait_times (tuple of ints): min and max time to wait between requests\n",
    "        csv_filename (str): the name of the csv file to store post and comment data\n",
    "        max_com (int): limits the number of comments per post to scrape\n",
    "    Returns\n",
    "        all_post_soups (list): list of soups for each post\n",
    "        Returns a list of the soups for each post scraped\n",
    "    '''\n",
    "    min_time, max_time = wait_times\n",
    "    all_post_soups = []\n",
    "    \n",
    "    # create CSV file\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # initialize column names if blank\n",
    "        if csvfile.tell() == 0:\n",
    "            csv_writer.writerow([\"user\", \"user_flair\", \"title\", \"post_text\", \n",
    "                                 \"post_date\", \"post_flair\", \"score\", \n",
    "                                 \"n_comments\", \"link\", \"is_comment\"])\n",
    "        \n",
    "        # iterate over all webpage soups\n",
    "        for page_soup in page_soups:\n",
    "            site_table = page_soup.find(\"div\", id=\"siteTable\")\n",
    "            posts = site_table.find_all(\"div\", class_=re.compile(r\"thing\"))\n",
    "            posts_links = [p.find(\"a\", class_= re.compile(\"bylink\")).get(\"href\") for p in posts]\n",
    "\n",
    "            for post_link in posts_links:\n",
    "                time.sleep(random.uniform(min_time, max_time))\n",
    "                \n",
    "                request = requests.get(post_link, headers=r_headers)\n",
    "\n",
    "                if request.status_code == 200:\n",
    "\n",
    "                    # find the post\n",
    "                    post_soup = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                    post = post_soup.find(\"div\", class_=re.compile(r\"sitetable\"))\n",
    "                    post_attrs = find_post_attrs(post)\n",
    "                    all_post_soups.append(post_soup)\n",
    "                    csv_writer.writerow(post_attrs + [False])\n",
    "\n",
    "                    # print(\"\\t### MOVING ON TO COMMENT SECTION ###\")\n",
    "                    \n",
    "                    # find the comment section\n",
    "                    comment_section = post_soup.find(\"div\", class_= \"sitetable nestedlisting\")\n",
    "                    comments = comment_section.find_all(\"div\", class_=\"entry unvoted\")\n",
    "\n",
    "                    for comment in comments[:max_com]:\n",
    "                        com_attrs = find_com_attrs(comment, post_link)\n",
    "                        \n",
    "                        csv_writer.writerow(com_attrs + [True])\n",
    "                    \n",
    "                    # print(f\"\\t\\t## STOPPED ##\\n\\t\\t## MOVING TO NEXT POST ##\")\n",
    "\n",
    "                else:\n",
    "                    # if we fail to get request, find the link that did it\n",
    "                    print(f\"!!!! FAILURE !!!!\\nResponse {request.status_code}. Couldn't get URL: {post_link}.\")                \n",
    "    return all_post_soups\n",
    "\n",
    "\n",
    "def get_data(domain_URL, r_headers, wait_times, max_pages, max_com, pages_csv, posts_csv):\n",
    "    \"\"\"\n",
    "    Given a subreddit url, find all posts and comments within the time frame\n",
    "    for debugging purposes, returns soups for all posts\n",
    "\n",
    "    inputs:\n",
    "        domain_URL (str): the URL of the subreddit\n",
    "        r_headers (dict): headers for the request\n",
    "        wait_times (tuple of ints): min and max time to wait between requests\n",
    "        max_pages (int): limits the number of pages to crawl\n",
    "        max_com (int): limits the number of comments per post to scrape\n",
    "        pages_csv (str): the name of the csv file to store page soups\n",
    "        posts_csv (str): the name of the csv file to store post and comment data\n",
    "\n",
    "    Returns soups for all pages and creates two CSV files \n",
    "        one for tracking page soups, the other for the actual data\n",
    "\n",
    "    \"\"\"\n",
    "    # get soup for EACH webpage in a subreddit\n",
    "    _, soups, _ = reddit_crawler(domain_URL, r_headers, wait_times, max_pages, pages_csv)\n",
    "\n",
    "    print(\"##### DONE CRAWLING #####\\nMOVING ON TO SCRAPING FOR POSTS AND COMMENTS\")\n",
    "\n",
    "    post_soups = reddit_scraper(soups, r_headers, wait_times, posts_csv, max_com)\n",
    "    \n",
    "    print(\"##### DONE SCRAPING #####\")\n",
    "    \n",
    "    return soups, post_soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages visited: 1\n",
      "Pages visited: 2\n",
      "Pages visited: 3\n",
      "Pages visited: 4\n",
      "Pages visited: 5\n",
      "Pages visited: 6\n",
      "Pages visited: 7\n",
      "Pages visited: 8\n",
      "Pages visited: 9\n",
      "Pages visited: 10\n",
      "Pages visited: 11\n",
      "Pages visited: 12\n",
      "Pages visited: 13\n",
      "Pages visited: 14\n",
      "Pages visited: 15\n",
      "Pages visited: 16\n",
      "Pages visited: 17\n",
      "Pages visited: 18\n",
      "Pages visited: 19\n",
      "Pages visited: 20\n",
      "Pages visited: 21\n",
      "Pages visited: 22\n",
      "Pages visited: 23\n",
      "Pages visited: 24\n",
      "Pages visited: 25\n",
      "Pages visited: 26\n",
      "Pages visited: 27\n",
      "Pages visited: 28\n",
      "Pages visited: 29\n",
      "Pages visited: 30\n",
      "Pages visited: 31\n",
      "Pages visited: 32\n",
      "Pages visited: 33\n",
      "Pages visited: 34\n",
      "Pages visited: 35\n",
      "Pages visited: 36\n",
      "Pages visited: 37\n",
      "Pages visited: 38\n",
      "Pages visited: 39\n",
      "Pages visited: 40\n",
      "##### DONE CRAWLING #####\n",
      "MOVING ON TO SCRAPING FOR POSTS AND COMMENTS\n",
      "##### DONE SCRAPING #####\n"
     ]
    }
   ],
   "source": [
    "# RUN FOR REAL\n",
    "soups = get_data(\"https://old.reddit.com/r/Adopted/\", {\"User-Agent\": \"Ethan K.\"}, (1, 2), 1000, 150, 'adopt_pages.csv', \"adopt_posts.csv\")\n",
    "\n",
    "pd.Series(soups).to_csv(\"all_adopted_posts_soups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages visited: 1\n",
      "Pages visited: 2\n",
      "Pages visited: 3\n",
      "Pages visited: 4\n",
      "Pages visited: 5\n",
      "Pages visited: 6\n",
      "Pages visited: 7\n",
      "Pages visited: 8\n",
      "Pages visited: 9\n",
      "Pages visited: 10\n",
      "Pages visited: 11\n",
      "Pages visited: 12\n",
      "Pages visited: 13\n",
      "Pages visited: 14\n",
      "Pages visited: 15\n",
      "Pages visited: 16\n",
      "Pages visited: 17\n",
      "Pages visited: 18\n",
      "Pages visited: 19\n",
      "Pages visited: 20\n",
      "Pages visited: 21\n",
      "Pages visited: 22\n",
      "Pages visited: 23\n",
      "Pages visited: 24\n",
      "Pages visited: 25\n",
      "Pages visited: 26\n",
      "Pages visited: 27\n",
      "Pages visited: 28\n",
      "Pages visited: 29\n",
      "Pages visited: 30\n",
      "Pages visited: 31\n",
      "Pages visited: 32\n",
      "Pages visited: 33\n",
      "Pages visited: 34\n",
      "Pages visited: 35\n",
      "Pages visited: 36\n",
      "Pages visited: 37\n",
      "Pages visited: 38\n",
      "Pages visited: 39\n",
      "Pages visited: 40\n",
      "##### DONE CRAWLING #####\n",
      "MOVING ON TO SCRAPING FOR POSTS AND COMMENTS\n",
      "##### DONE SCRAPING #####\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "soups2 = get_data(\"https://old.reddit.com/r/Adoption/\", {\"User-Agent\": \"Ethan Koz\"}, (1.1, 2.1), 1000, 1500, 'adoption_pages.csv', \"adoption_posts.csv\")\n",
    "pd.Series(soups2).to_csv(\"all_adoption_posts_soups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://old.reddit.com/r/Adoption/?count=975&after=t3_15u1ekj'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"adoption_pages.csv\").loc[39,\"link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups3 = get_data(\"https://old.reddit.com/r/Adoption/\", {\"User-Agent\": \"Ethan Koz\"}, (1.1, 2.1), 1000, 1500, 'adoption_pages.csv', \"adoption_posts.csv\")\n",
    "pd.Series(soups3).to_csv(\"all_adoption_posts_soups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding all posts on second website\n",
    "# soups[1]\n",
    "# site_table = soups[1].find(\"div\", id=\"siteTable\")\n",
    "# posts = site_table.find_all(\"div\", class_=re.compile(r\"thing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = [post.find(\"a\", class_= re.compile(\"bylink\")).get(\"href\") for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD ###\n",
    "\n",
    "# headers = {\"User-Agent\": \"For school practice, ethanjkozlowski@uchicago.edu\"}\n",
    "# skipped = []\n",
    "# all_post_soups = []\n",
    "\n",
    "# with open(\"all_posts_new_1_30.csv\", 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#     csv_writer = csv.writer(csvfile)\n",
    "#     if csvfile.tell() == 0:\n",
    "#         csv_writer.writerow([\"user\", \"user_flair\", \"title\", \"post_text\", \n",
    "#                   \"post_date\", \"post_flair\", \"score\", \"n_comments\", \"link\"])\n",
    "    \n",
    "#     for soup in soups:\n",
    "#         site_table = soup.find(\"div\", id=\"siteTable\")\n",
    "#         posts = site_table.find_all(\"div\", class_=re.compile(r\"thing\"))\n",
    "#         links = [post.find(\"a\", class_= re.compile(\"bylink\")).get(\"href\") for post in posts]\n",
    "\n",
    "#         for link in links:\n",
    "#             time.sleep(1.8)\n",
    "#             request = requests.get(link, headers = headers)\n",
    "            \n",
    "#             if request.status_code == 200:\n",
    "#                 soup_2 = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "#                 post = soup_2.find(\"div\", class_=re.compile(r\"sitetable\"))\n",
    "#                 post_attrs = find_post_attrs(post)\n",
    "#                 all_post_soups.append(soup_2)\n",
    "#                 csv_writer.writerow(post_attrs)\n",
    "#             else:\n",
    "#                 skipped.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### OLD ####\n",
    "# def reddit_crawler(domain_URL, r_headers, wait_time, max_pages, csv_filename):\n",
    "#     \"\"\"\n",
    "#     Crawls a subreddit for all posts, and comments\n",
    "#     \"\"\"\n",
    "    \n",
    "#     all_posts = []\n",
    "#     visited_urls = set()\n",
    "\n",
    "#     # add the starting URL to the queue\n",
    "#     curr_url = domain_URL\n",
    "    \n",
    "#     # code improved using Chat GPT3.5\n",
    "#     # prompt: how to dynamically update a csv while you scrape a website\n",
    "#     with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#         # Create a CSV writer object\n",
    "#         csv_writer = csv.writer(csvfile)\n",
    "#         # Check if the file is empty (write header if needed)\n",
    "#         if csvfile.tell() == 0:\n",
    "#             csv_writer.writerow(['user',\n",
    "#                                  'user_flair',\n",
    "#                                  'title',\n",
    "#                                  \"post_link\"\n",
    "#                                  'post_text'\n",
    "#                                  'post_date'\n",
    "#                                  'post_flair'\n",
    "#                                  'score',\n",
    "#                                  \"n_comments\",\n",
    "#                                  ])\n",
    "        \n",
    "#         while len(visited_urls) < max_pages:\n",
    "#             time.sleep(wait_time)\n",
    "#             request = requests.get(curr_url, headers = r_headers)\n",
    "#             links = []\n",
    "            \n",
    "#             if request.status_code == 200:\n",
    "#                 soup = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                \n",
    "\n",
    "#                 site_table = soup.find(\"div\", id=\"siteTable\")\n",
    "#                 posts = site_table.find_all(\"div\", class_=re.compile(r\"thing\"))\n",
    "#                 links = [post.find(\"a\", class_= re.compile(\"bylink\")).get(\"href\") for post in posts]\n",
    "\n",
    "#                 for link in links:\n",
    "#                     time.sleep(wait_time)\n",
    "#                     request = requests.get(link, headers = r_headers)\n",
    "                    \n",
    "#                     if request.status_code == 200:\n",
    "#                         soup = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                        \n",
    "#                         post = find_post_attrs(soup)\n",
    "#                         # Write the scraped data to the CSV file\n",
    "#                         csv_writer.writerow(post)\n",
    "#                         all_posts.append(post)\n",
    "\n",
    "#                 curr_url = get_next_page(soup)\n",
    "                                    \n",
    "#             else:\n",
    "#                 print(f\"Request could not be retrieved: {request}\")\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing function\n",
    "# headers = {\"User-Agent\": \"student practice\"}\n",
    "# a = reddit_crawler(\"https://old.reddit.com/r/Adopted/\", headers, 10, 2, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for link in problem_urls:\n",
    "#     request = requests.get(link, headers = headers)\n",
    "\n",
    "#     if request.status_code == 200:\n",
    "#         soup = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "#         time.sleep(2)\n",
    "#         print(find_post_attrs(soup))\n",
    "# # 0 user, 1 user_flair, 2 title, 3 post_text, 4 post_date, 5 post_flair, 6 score, 7 n_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find(\"div\", class_=re.compile(r\"sitetable\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"all_posts_with_href.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
