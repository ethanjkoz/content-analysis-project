{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in your implementation, we recommend you to:\n",
    "\n",
    "*    explore both DT and RF models\n",
    "*    explore different parameter settings to find a model that works best on your data\n",
    "*    comment on important steps\n",
    "*    interpret model performance\n",
    "*    discuss pros and cons of each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important modules for import ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for counting\n",
    "from collections import Counter\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IGNORE THIS SET UP: only needed to run once to save the file for later use ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DON'T RUN AGAIN ####\n",
    "# # for this task, merge them\n",
    "\n",
    "\n",
    "# # import the correct csv files from another file\n",
    "# import sys\n",
    "# sys.path.append(\"D:\\\\hw\\\\adopt-proj\\\\\")\n",
    "# from adopt_setup import adopted_posts, adopted_comms, adoption_posts, adoption_comms\n",
    "\n",
    "# # get the dataframes\n",
    "# adp_df = adopted_posts\n",
    "# adc_df = adopted_comms\n",
    "# anp_df = adoption_posts\n",
    "# anc_df = adoption_comms\n",
    "\n",
    "# # concat all the dfs\n",
    "# all_df = pd.concat([adp_df, adc_df, anp_df, anc_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ### function from HW2 of Content Analysis ###\n",
    "\n",
    "\n",
    "# def word_tokenize(word_list):\n",
    "#     tokenized = []\n",
    "#     # pass word list through language model.\n",
    "#     doc = nlp(word_list)\n",
    "#     for token in doc:\n",
    "#         if not token.is_punct and len(token.text.strip()) > 0:\n",
    "#             tokenized.append(token.text)\n",
    "#     return tokenized\n",
    "\n",
    "\n",
    "# def normalizeTokens(word_list, extra_stop=[]):\n",
    "#     #We can use a generator here as we just need to iterate over it\n",
    "#     normalized = []\n",
    "#     if type(word_list) == list and len(word_list) == 1:\n",
    "#         word_list = word_list[0]\n",
    "\n",
    "#     if type(word_list) == list:\n",
    "#         word_list = ' '.join([str(elem) for elem in word_list])\n",
    "\n",
    "#     doc = nlp(word_list.lower())\n",
    "\n",
    "#     # add the property of stop word to words considered as stop words\n",
    "#     if len(extra_stop) > 0:\n",
    "#         for stopword in extra_stop:\n",
    "#             lexeme = nlp.vocab[stopword]\n",
    "#             lexeme.is_stop = True\n",
    "\n",
    "#     for w in doc:\n",
    "#         # if it's not a stop word or punctuation mark, add it to our article\n",
    "#         if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "#             # we add the lematized version of the word\n",
    "#             normalized.append(str(w.lemma_))\n",
    "\n",
    "#     return normalized\n",
    "\n",
    "\n",
    "# ### TOKENIZE AND NORMALIZE WORDS ###\n",
    "# # takes around 66 minutes to run\n",
    "# all_df[\"tokenized_text\"] = all_df.loc[:,'post_text'].apply(lambda x: word_tokenize(x))\n",
    "# all_df['word_counts'] = all_df.loc[:,'tokenized_text'].apply(lambda x: len(x))\n",
    "# # takes around 61 minutes to run\n",
    "# all_df['normalized_tokens'] = all_df['tokenized_text'].apply(lambda x: normalizeTokens(x))\n",
    "# all_df['normalized_tokens_count'] = all_df['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "# # drop duplicates\n",
    "# all_df = all_df.drop_duplicates(subset=['post_text', \"user\"])\n",
    "# all_df.reset_index(drop=True)\n",
    "\n",
    "# need to remove \n",
    "# all_df.loc[all_df[\"user_flair\"] == \"nan\",\"is_adoptee\"] = np.nan # might need to add .astype(str)\n",
    "# all_df.loc[all_df[\"user_flair\"] == \"nan\",\"user_flair\"] = np.nan # might need to add .astype(str)\n",
    "\n",
    "# # save data as pkl file (to keep data type, as opposed to csv which casts all to str)\n",
    "# all_df.to_pickle(\"all_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS is where the code begins ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT OUR CLEANED DATA ##\n",
    "all_df = pd.read_pickle(\"D:\\\\hw\\\\adopt-proj\\\\newest_all_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user                  979\n",
       "user_flair         247339\n",
       "post_date               0\n",
       "post_flair         297083\n",
       "score                  74\n",
       "n_comments         287462\n",
       "link                51459\n",
       "is_comment              0\n",
       "subreddit               0\n",
       "full_text               0\n",
       "cleaner_text            0\n",
       "full_tokens             0\n",
       "word_count              0\n",
       "norm_tokens             0\n",
       "tokens_sents            0\n",
       "norm_sents              0\n",
       "POS_sents               0\n",
       "is_adoptee              0\n",
       "num_tokens              0\n",
       "num_norm_tokens         0\n",
       "sentiment               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of nulls in each column\n",
    "# since most text is from comments, they do not have title, n_comments, or post_flair\n",
    "all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from TextClassification.ipynb\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expand the contracted phrase into normal words\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase) # prime \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    \"\"\"\n",
    "    Clean the review texts\n",
    "    \"\"\"\n",
    "    cleaned_post_text = []\n",
    "\n",
    "    for post_text in tqdm(df['post_text']):\n",
    "        \n",
    "        # expand the contracted words\n",
    "        post_text = decontracted(post_text)\n",
    "        \n",
    "        #remove html tags\n",
    "        post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        #remove non-alphabetic characters\n",
    "        post_text = re.sub(\"[^a-zA-Z]\",\" \", post_text)\n",
    "    \n",
    "        #remove url \n",
    "        post_text = re.sub(r'https?://\\S+|www\\.\\S+', '', post_text)\n",
    "        \n",
    "        #Removing punctutation, string.punctuation in python consists of !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
    "        post_text = post_text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # ''.join([char for char in movie_text_data if char not in string.punctuation])\n",
    "        \n",
    "        # remove emails\n",
    "        post_text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", '', post_text)\n",
    "    \n",
    "        cleaned_post_text.append(post_text)\n",
    "\n",
    "    return cleaned_post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df['cleaner_text'] = clean_text(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find the subset of the data that contains labeled data, the ground truth. We are assuming here that users who add flairs to their names are doing so truthfully. All flairs were checked using regular expressions and manually inspected to determine whether each use was an adoptee or not. Furthermore, any flair that could not be determined to be some form of adoptee, which includes adoptive parents, etc. was coded as False. All posts who had no attached user flair were deemed NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[all_df.num_tokens > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_df = all_df[all_df.is_adoptee < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHanges \"is_adoptee\" bools to binary 0 and 1 in a new column called \"labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find our stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GETTING INTO MACHINE LEARNING, VECTORIZATION, DECISION TREES, ETC ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to tune the max_df, min_df parameters and possibly add more stop words. For this example, I will use a count vectorizer (bag of words) for this classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words, max_df=0.9, min_df=3, ngram_range=(1,1))\n",
    "\n",
    "# convert the cleaned reviews to vectors\n",
    "X = vectorizer.fit_transform(truth_df.cleaner_text)\n",
    "y = truth_df.is_adoptee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test splitting ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X_train : (17021, 16016), y_train : (17021,)\n",
      "Testing data: X_test : (7296, 16016), y_test : (7296,)\n"
     ]
    }
   ],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(truth_df.shape[0]), test_size=0.3, \n",
    "                                       shuffle=True, random_state=42)\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "\n",
    "X_test = X[test_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "print(\"Training data: X_train : {}, y_train : {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Testing data: X_test : {}, y_test : {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training data is 70% where as testing is 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data distribution: 7019 (not-adoptee), 10002 (adoptee)\n",
      "Testing data distribution: 2982 (not-adoptee), 4314 (adoptee)\n"
     ]
    }
   ],
   "source": [
    "count_y_train = Counter(y_train)\n",
    "count_y_test = Counter(y_test)\n",
    "print(f\"Training data distribution: {count_y_train[0]} (not-adoptee), {count_y_train[1]} (adoptee)\")\n",
    "print(f\"Testing data distribution: {count_y_test[0]} (not-adoptee), {count_y_test[1]} (adoptee)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also note that the distributions for testing and training regarding adoptee and non adoptee are roughly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING A DECISION TREE MODEL ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the DT classifier correctly classifies the texts to the correct label in .65 of all cases. I will now change some of the DT parameters to see if this improves model fit. This model only barely does better than a model that would predict all posts as being written by an adoptee. I fear that not setting max_depth has lead to the tree overfitting the training data. Therefore it is important to tune parameters for the DT classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65228\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"gini\",random_state=42)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after limiting the max_depth to 12, changing splitting criterion to entropy, setting max_leaf_notes to 100, and setting max features to a proportion of .8; the fit has improved (.682) to be slightly better than 2/3. Also I tinkered around with other settings, but I couldn't drastically improve the model more than this. Changing the max features, max depth, and max leaf nodes probably reduced the ability of the tree from overfitting the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68503\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=12,\n",
    "                                random_state=42, max_leaf_nodes=100,\n",
    "                                max_features=.8)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after adjusting for class weights (since there are about double the amount of non adoptees as adoptees in the training and testing splits), our model performs worse again (.647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67859\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=12,\n",
    "                                random_state=42, max_leaf_nodes=100,\n",
    "                                max_features=.8, class_weight=\"balanced\")\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this model is using text data, getting at which features are important for the model is made more difficult. I am not totally sure how I would go about doing this for text data where the features are individual word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predprob_test = dt_clf.predict_proba(X_test)\n",
    "probs = pd.DataFrame(y_predprob_test[1:], columns= [\"predictprob1\", \"predict_prob0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.58      0.59      2982\n",
      "           1       0.72      0.75      0.73      4314\n",
      "\n",
      "    accuracy                           0.68      7296\n",
      "   macro avg       0.67      0.66      0.66      7296\n",
      "weighted avg       0.68      0.68      0.68      7296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = dt_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table we can see that the recall for 1 is quite low (of all adoptee posts, only 35% were labeled as such). The precision is also low (less than half of all posts classified as being adopted actually were). Overall, the f1-score is also low for 1. This means that the model performs worse than random chance. For 0, non-adopted posts, precision, recall, and f1 score are generally much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a different method for word vectorizer ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=stop_words, max_df=0.9, min_df=3, ngram_range=(1,2))\n",
    "\n",
    "# convert the cleaned reviews to vectors\n",
    "X = vectorizer.fit_transform(truth_df.cleaner_text)\n",
    "y = truth_df.is_adoptee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(truth_df.shape[0]), test_size=0.3, \n",
    "                                       shuffle=True, random_state=42)\n",
    "X_train = X[train_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reapply the classifier and fit the model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the same classifier criteria, the DT using tf-idf vectorizer has slightly improved the overall model performance compared to countevectorizer. To note, these models take much longer to run than countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64556\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"gini\",random_state=42)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust the classifier hyperparams ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, compared to the best model using countvectorizer, using tf-idf vectorizer has very slightly improved the model performance. The model will correctly guess that a post is made by an adoptee or not in 68.21% of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68147\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=12,\n",
    "                                random_state=42, max_leaf_nodes=100,\n",
    "                                max_features=.8, class_weight=\"balanced\")\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to improve this model by adjusting the parameters in different ways ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best tuned model I could get by changing around the hyperparameters. It seems that a combination of changing the splitting criterion to GINI, increasing the max depth to 20, and reducing the max_features to half of all features (.5) have produced the best results. Evidently, this model produces a score of .684, the best yet. It will correctly label posts in 68.42% of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68353\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=20,\n",
    "                                random_state=42, max_leaf_nodes=100,\n",
    "                                max_features=.5, class_weight=\"balanced\")\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# then predict on test set\n",
    "print(float(\"{:.5f}\".format(dt_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61      2982\n",
      "           1       0.73      0.73      0.73      4314\n",
      "\n",
      "    accuracy                           0.68      7296\n",
      "   macro avg       0.67      0.67      0.67      7296\n",
      "weighted avg       0.68      0.68      0.68      7296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = dt_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that compared to the other vectorizer, tf-idf performs better at precision for adoptee posts, but worse for non adoptees. Recall for non adoptee posts is .95, which means that almost all non adoptee posts were labeled correctly. The f1 score for non adoptee posts is slightly higher than countvectorizer at .80 (vs .75). However, this model fairs worse for recall and f1 score of adoptee posts. Recall for adoptee posts is half of that of the other model (.16 vs .35). Furthermore the f1-score has decreased as a result as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FORESTS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the accuracy of the other model using the tf-idf vectorizer was better, the f1-score was much worse. Therefore let us go back to using countvectorizer for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words, max_df=0.9, \n",
    "                             min_df=3, ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(truth_df.cleaner_text)\n",
    "y = truth_df.is_adoptee\n",
    "train_idx, test_idx = train_test_split(np.arange(truth_df.shape[0]), test_size=0.3, \n",
    "                                       shuffle=True, random_state=42)\n",
    "X_train = X[train_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74973\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', \n",
    "                                random_state = 42, class_weight=\"balanced\")\n",
    "\n",
    "# train RFC\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the random forest classifier on test set \n",
    "print(float(\"{:.5f}\".format(rf_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.56      0.65      2982\n",
      "           1       0.74      0.88      0.81      4314\n",
      "\n",
      "    accuracy                           0.75      7296\n",
      "   macro avg       0.75      0.72      0.73      7296\n",
      "weighted avg       0.75      0.75      0.74      7296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is an improvement over any of the classifiers performed thus far. The accuracy of the model is 70% meaning that it accurately labels adoptee and non-adoptee posts 69.7% of the time. Which is still not super good, considering if one predicted all posts were made by non adoptees, the accuracy of the model would still be 66.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with the other vectorizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=stop_words, max_df=0.9, \n",
    "                             min_df=3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(truth_df.cleaner_text)\n",
    "y = truth_df.is_adoptee\n",
    "train_idx, test_idx = train_test_split(np.arange(truth_df.shape[0]), test_size=0.3, \n",
    "                                       shuffle=True, random_state=42)\n",
    "X_train = X[train_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74904\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', \n",
    "                                random_state = 42, class_weight=\"balanced\")\n",
    "\n",
    "# train RFC\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the random forest classifier on test set \n",
    "print(float(\"{:.5f}\".format(rf_clf.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.55      0.64      2982\n",
      "           1       0.74      0.89      0.81      4314\n",
      "\n",
      "    accuracy                           0.75      7296\n",
      "   macro avg       0.75      0.72      0.72      7296\n",
      "weighted avg       0.75      0.75      0.74      7296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the precision is actually better for the adoptee posts this time. Otherwise the recall and f1 score fairly similar to the other random forest with countvectorizer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
